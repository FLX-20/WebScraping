{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect_langs\n",
    "import re\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of inappropriate words\n",
    "with open('en.txt', 'r') as file:\n",
    "    bad_words = set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_english(text):\n",
    "    try:\n",
    "        probabilities = detect_langs(text)\n",
    "        for language in probabilities:\n",
    "            if language.lang == 'en' and language.prob > 0.99:\n",
    "                return True, ''\n",
    "        return False, 'Text is not in English or confidence is below 99%.'\n",
    "    except Exception as e:\n",
    "        return False, f'Language detection failed: {e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cleanliness(text):\n",
    "    words_in_text = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "    for word in bad_words:\n",
    "        if word in words_in_text:\n",
    "            return False, f'Text contains inappropriate whole word: {word}'\n",
    "    return True, ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wikipedia(term):\n",
    "\n",
    "    ############ data acquisition #######################\n",
    "    # compose the url for the search term\n",
    "    url = f\"https://en.wikipedia.org/wiki/{term}\"\n",
    "\n",
    "    # get the response from the server\n",
    "    response = requests.get(url)\n",
    "\n",
    "    ############# text extraction #######################\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    errors = []\n",
    "    text = \"\"\n",
    "    \n",
    "    for paragraph in soup.find_all('p'):\n",
    "        # count the number of words in each paragraph\n",
    "        words = paragraph.get_text().strip().split()\n",
    "        # if the paragraph has less than 5 words, skip it\n",
    "        if len(words) < 5:\n",
    "            continue\n",
    "        \n",
    "        text  = text + paragraph.get_text().strip() + \"\\n\"\n",
    "    \n",
    "    ############# bracket filter #######################\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "    ############# word number filter ###################\n",
    "    sentences = text.split(\".\")\n",
    "    sentences = [sentence + \".\" for sentence in sentences]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) < 3:\n",
    "            # remove sentences that are shorter than 3 words\n",
    "            text = text.replace(sentence, \"\")\n",
    "\n",
    "    ############# sentence number filter #################\n",
    "    if len(text.strip().split(\".\")) < 10:\n",
    "        errors.append(\"Document has less than 10 sentences.\")\n",
    "   \n",
    "    ############# language filter ########################\n",
    "    is_english, error_msg = check_english(text)\n",
    "    if not is_english:\n",
    "        errors.append(f'Language Error: {error_msg}')\n",
    "\n",
    "    ############# bad word filter ########################\n",
    "    is_clean, error_msg = check_cleanliness(text)\n",
    "    if not is_clean:\n",
    "        errors.append(f'Cleanliness Error: {error_msg}')\n",
    "\n",
    "    return text, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the term to be searched\n",
    "term = \"car\"\n",
    "data, error_messages = crawl_wikipedia(term)\n",
    "\n",
    "# appand the data to a XML file with index number, text, errors and website\n",
    "with open(\"wiki.xml\", \"a\") as file:\n",
    "    file.write(f\"<document>\\n<id>{term}</id>\\n<text>{data}</text>\\n<errors>{error_messages}</errors>\\n<website>https://en.wikipedia.org/wiki/{term}</website>\\n</document>\\n\")\n",
    "\n",
    "\n",
    "# Print errors\n",
    "for error in error_messages:\n",
    "    print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
